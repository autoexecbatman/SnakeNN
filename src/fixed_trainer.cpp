#include \"neural_network.h\"\n#include \"snake_logic.h\"\n#include <iostream>\n\n// Fixed trainer with simplified architecture and immediate learning\nclass FixedDQNTrainer {\npublic:\n    FixedDQNTrainer() : network(8, 32, 4), target_network(8, 32, 4) {\n        // Copy initial weights\n        auto source_params = network.parameters();\n        auto target_params = target_network.parameters();\n        torch::NoGradGuard no_grad;\n        for (size_t i = 0; i < source_params.size(); i++) {\n            target_params[i].copy_(source_params[i]);\n        }\n    }\n    \n    void train(int episodes = 1000, bool visual = false) {\n        std::cout << \"=== FIXED TRAINER (No Experience Replay) ===\" << std::endl;\n        std::cout << \"Network: 8->32->4 | Learning Rate: 0.01 | Immediate Training\" << std::endl;\n        \n        float epsilon = 1.0f;\n        const float epsilon_decay = 0.999f;\n        const float epsilon_min = 0.05f;\n        \n        int total_score = 0;\n        int successful_episodes = 0;\n        \n        for (int episode = 0; episode < episodes; episode++) {\n            SnakeGame game;\n            game.reset();\n            \n            std::vector<float> prev_state;\n            int prev_action = -1;\n            float total_reward = 0.0f;\n            int steps = 0;\n            \n            while (!game.isGameOver() && steps < 500) {\n                auto current_state = getSimplifiedState(game);\n                int action = getAction(current_state, epsilon);\n                \n                // Take action\n                game.setDirection(static_cast<Direction>(action));\n                bool game_continues = game.update();\n                \n                float reward = getImprovedReward(game, !game_continues);\n                total_reward += reward;\n                steps++;\n                \n                // IMMEDIATE TRAINING - No experience replay!\n                if (!prev_state.empty()) {\n                    trainStep(prev_state, prev_action, reward, current_state, !game_continues);\n                }\n                \n                prev_state = current_state;\n                prev_action = action;\n                \n                if (!game_continues) break;\n            }\n            \n            int score = game.getScore();\n            total_score += score;\n            if (score > 0) successful_episodes++;\n            \n            // Update target network frequently\n            if (episode % 25 == 0) {\n                updateTargetNetwork();\n            }\n            \n            // Slower epsilon decay\n            if (epsilon > epsilon_min) {\n                epsilon *= epsilon_decay;\n            }\n            \n            // Detailed progress reporting\n            if (episode % 50 == 0) {\n                float avg_score = (float)total_score / (episode + 1);\n                float success_rate = (float)successful_episodes / (episode + 1) * 100.0f;\n                \n                std::cout << \"Episode \" << episode << \"/\" << episodes << std::endl;\n                std::cout << \"  Score this episode: \" << score << std::endl;\n                std::cout << \"  Average score: \" << avg_score << std::endl;\n                std::cout << \"  Success rate: \" << success_rate << \"%\" << std::endl;\n                std::cout << \"  Total reward: \" << total_reward << std::endl;\n                std::cout << \"  Steps: \" << steps << std::endl;\n                std::cout << \"  Epsilon: \" << epsilon << std::endl;\n                std::cout << \"  Recent scores: \";\n                \n                // Show last 10 scores\n                for (int i = std::max(0, episode - 9); i <= episode; i++) {\n                    // This is simplified - in real implementation you'd store these\n                    std::cout << (i == episode ? score : 0) << \" \";\n                }\n                std::cout << std::endl << std::endl;\n            }\n            \n            // Early success detection\n            if (episode >= 100 && episode % 100 == 0) {\n                float recent_avg = (float)total_score / (episode + 1);\n                if (recent_avg > 1.5f) {\n                    std::cout << \"*** EARLY SUCCESS DETECTED! Average score > 1.5 ***\" << std::endl;\n                }\n            }\n        }\n        \n        float final_avg = (float)total_score / episodes;\n        float final_success_rate = (float)successful_episodes / episodes * 100.0f;\n        \n        std::cout << std::endl << \"=== FINAL RESULTS ===\" << std::endl;\n        std::cout << \"Total episodes: \" << episodes << std::endl;\n        std::cout << \"Total score: \" << total_score << std::endl;\n        std::cout << \"Average score: \" << final_avg << std::endl;\n        std::cout << \"Successful episodes: \" << successful_episodes << std::endl;\n        std::cout << \"Success rate: \" << final_success_rate << \"%\" << std::endl;\n        \n        if (final_avg > 1.0f) {\n            std::cout << \"*** SUCCESS! Snake learned to find food! ***\" << std::endl;\n        } else if (final_avg > 0.3f) {\n            std::cout << \"** PARTIAL SUCCESS - Some learning occurred **\" << std::endl;\n        } else {\n            std::cout << \"* FAILURE - Little to no learning *\" << std::endl;\n        }\n    }\n    \nprivate:\n    SnakeNeuralNetwork network;\n    SnakeNeuralNetwork target_network;\n    const float learning_rate = 0.01f;  // 10x higher than original\n    const float gamma = 0.95f;\n    \n    std::vector<float> getSimplifiedState(const SnakeGame& game) {\n        auto head = game.getSnakeBody()[0];\n        auto food = game.getFoodPosition();\n        \n        std::vector<float> state(8);\n        \n        // Food direction (4 features)\n        state[0] = (food.x > head.x) ? 1.0f : 0.0f;  // Food right\n        state[1] = (food.x < head.x) ? 1.0f : 0.0f;  // Food left  \n        state[2] = (food.y > head.y) ? 1.0f : 0.0f;  // Food down\n        state[3] = (food.y < head.y) ? 1.0f : 0.0f;  // Food up\n        \n        // Immediate danger (4 features)\n        for (int i = 0; i < 4; i++) {\n            Direction testDir = static_cast<Direction>(i);\n            Position testPos = head;\n            \n            switch (testDir) {\n                case Direction::UP: testPos.y--; break;\n                case Direction::DOWN: testPos.y++; break;\n                case Direction::LEFT: testPos.x--; break;\n                case Direction::RIGHT: testPos.x++; break;\n            }\n            \n            bool danger = (testPos.x < 0 || testPos.x >= SnakeGame::GRID_WIDTH ||\n                          testPos.y < 0 || testPos.y >= SnakeGame::GRID_HEIGHT ||\n                          checkCollision(testPos, game.getSnakeBody()));\n            state[4 + i] = danger ? 1.0f : 0.0f;\n        }\n        \n        return state;\n    }\n    \n    int getAction(const std::vector<float>& state, float epsilon) {\n        if ((rand() % 1000) / 1000.0f < epsilon) {\n            return rand() % 4;\n        }\n        \n        auto action_tensor = network.getAction(state, 0.0f);\n        return static_cast<int>(action_tensor.cpu().item<int64_t>());\n    }\n    \n    float getImprovedReward(const SnakeGame& game, bool died) {\n        if (died) return -10.0f;\n        \n        static Position last_head(-1, -1);\n        static Position last_food(-1, -1);\n        \n        auto head = game.getSnakeBody()[0];\n        auto food = game.getFoodPosition();\n        \n        float reward = 0.1f;  // Base survival reward\n        \n        // Big reward for eating food\n        if (last_food.x >= 0 && head.x == last_food.x && head.y == last_food.y) {\n            reward += 10.0f;\n        }\n        \n        // Distance-based reward\n        if (last_head.x >= 0) {\n            float old_dist = abs(last_head.x - food.x) + abs(last_head.y - food.y);\n            float new_dist = abs(head.x - food.x) + abs(head.y - food.y);\n            \n            if (new_dist < old_dist) {\n                reward += 0.5f;  // Reward for getting closer\n            } else if (new_dist > old_dist) {\n                reward -= 0.1f;  // Small penalty for getting farther\n            }\n        }\n        \n        last_head = head;\n        last_food = food;\n        \n        return reward;\n    }\n    \n    void trainStep(const std::vector<float>& state, int action, float reward,\n                   const std::vector<float>& next_state, bool done) {\n        \n        auto state_tensor = torch::zeros({1, 8}, torch::kFloat);\n        auto next_state_tensor = torch::zeros({1, 8}, torch::kFloat);\n        \n        for (int i = 0; i < 8; i++) {\n            state_tensor[0][i] = state[i];\n            next_state_tensor[0][i] = next_state[i];\n        }\n        \n        auto current_q = network.forward(state_tensor);\n        \n        float target_q = reward;\n        if (!done) {\n            auto next_q = target_network.forward(next_state_tensor);\n            target_q += gamma * std::get<0>(next_q.max(1)).cpu().item<float>();\n        }\n        \n        auto target_tensor = current_q.clone();\n        target_tensor[0][action] = target_q;\n        \n        auto loss = torch::mse_loss(current_q, target_tensor);\n        \n        // Immediate backprop with higher learning rate\n        loss.backward();\n        \n        auto params = network.parameters();\n        torch::NoGradGuard no_grad;\n        for (auto& param : params) {\n            if (param.grad().defined()) {\n                param -= learning_rate * param.grad();\n                param.grad().zero_();\n            }\n        }\n    }\n    \n    void updateTargetNetwork() {\n        auto source_params = network.parameters();\n        auto target_params = target_network.parameters();\n        torch::NoGradGuard no_grad;\n        for (size_t i = 0; i < source_params.size(); i++) {\n            target_params[i].copy_(source_params[i]);\n        }\n    }\n    \n    bool checkCollision(const Position& pos, const std::vector<Position>& snake) {\n        for (const auto& segment : snake) {\n            if (segment.x == pos.x && segment.y == pos.y) {\n                return true;\n            }\n        }\n        return false;\n    }\n};\n\nint main() {\n    std::cout << \"=== FIXED SNAKE AI TRAINER ===\" << std::endl;\n    \n    try {\n        FixedDQNTrainer trainer;\n        trainer.train(1000, false);  // 1000 episodes, no visualization\n        \n    } catch (const std::exception& e) {\n        std::cerr << \"Error: \" << e.what() << std::endl;\n        return -1;\n    }\n    \n    return 0;\n}\n
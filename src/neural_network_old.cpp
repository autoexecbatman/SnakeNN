#include \"neural_network.h\"\n#include <iostream>\n#include <random>\n#include <numeric>\n#include <algorithm>\n#include \"snake_logic.h\"\n\nSnakeNeuralNetworkImpl::SnakeNeuralNetworkImpl(int input_size, int hidden_size, int output_size) \n    : fc1(register_module(\"fc1\", torch::nn::Linear(input_size, hidden_size))),\n      fc2(register_module(\"fc2\", torch::nn::Linear(hidden_size, hidden_size))),\n      fc3(register_module(\"fc3\", torch::nn::Linear(hidden_size, output_size))),\n      rng(std::random_device{}()) {\n}\n\ntorch::Tensor SnakeNeuralNetworkImpl::forward(torch::Tensor x) {\n    x = torch::relu(fc1->forward(x));\n    x = torch::relu(fc2->forward(x));\n    x = fc3->forward(x);\n    return x;\n}\n\ntorch::Tensor SnakeNeuralNetworkImpl::getAction(const std::vector<float>& state, float epsilon) {\n    if (std::uniform_real_distribution<float>(0.0f, 1.0f)(rng) < epsilon) {\n        // Random action\n        return torch::randint(0, 4, {1});\n    }\n    \n    // Neural network action\n    torch::NoGradGuard no_grad;\n    auto state_tensor = torch::from_blob(const_cast<float*>(state.data()), \n                                       {1, static_cast<long>(state.size())}, \n                                       torch::kFloat);\n    auto q_values = forward(state_tensor);\n    return torch::argmax(q_values, 1);\n}\n\nvoid SnakeNeuralNetworkImpl::save(const std::string& path) {\n    torch::save(this->parameters(), path);\n}\n\nvoid SnakeNeuralNetworkImpl::load(const std::string& path) {\n    std::vector<torch::Tensor> params;\n    torch::load(params, path);\n    \n    auto model_params = this->parameters();\n    for (size_t i = 0; i < params.size() && i < model_params.size(); i++) {\n        model_params[i].data().copy_(params[i].data());\n    }\n}\n\nDQNTrainer::DQNTrainer(int input_size, int hidden_size, int output_size, \n                       float learning_rate, float gamma_param) {\n    gamma = gamma_param;\n    \n    // Initialize networks using assignment after construction\n    network = register_module(\"network\", SnakeNeuralNetwork(input_size, hidden_size, output_size));\n    target_network = register_module(\"target_network\", SnakeNeuralNetwork(input_size, hidden_size, output_size));\n    \n    // Initialize target network with same weights\n    auto source_params = network->named_parameters();\n    auto target_params = target_network->named_parameters();\n    torch::NoGradGuard no_grad;\n    for (auto& pair : source_params) {\n        target_params[pair.key()].copy_(pair.value());\n    }\n    \n    // Setup optimizer\n    optimizer = std::make_unique<torch::optim::Adam>(network->parameters(), torch::optim::AdamOptions(learning_rate));\n}\n\nvoid DQNTrainer::train(int episodes) {\n    // Force CPU for Release builds to avoid CUDA issues on Windows\n#ifdef NDEBUG\n    auto device = torch::kCPU;\n    std::cout << \"Release build: Using CPU only to avoid CUDA issues\" << std::endl;\n#else\n    auto device = torch::cuda::is_available() ? torch::kCUDA : torch::kCPU;\n    std::cout << \"Debug build: Training on: \" << (device == torch::kCUDA ? \"CUDA\" : \"CPU\") << std::endl;\n#endif\n    \n    network->to(device);\n    target_network->to(device);\n    \n    for (int episode = 0; episode < episodes; episode++) {\n        SnakeGame game;\n        auto state = game.getGameState();\n        float total_reward = 0.0f;\n        int steps = 0;\n        \n        while (!game.isGameOver() && steps < 1000) {\n            // Get action from network\n            auto action_tensor = network->getAction(state, epsilon);\n            int action = action_tensor.item<int>();\n            \n            // Convert action to direction\n            Direction dir = static_cast<Direction>(action);\n            game.setDirection(dir);\n            \n            // Take step\n            game.update();\n            auto next_state = game.getGameState();\n            float reward = game.getReward();\n            bool done = game.isGameOver();\n            \n            // Store experience\n            addExperience({state, action, reward, next_state, done});\n            \n            state = next_state;\n            total_reward += reward;\n            steps++;\n            \n            // Train network\n            if (replay_buffer.size() >= batch_size) {\n                replayTraining();\n            }\n        }\n        \n        // Decay epsilon\n        if (epsilon > epsilon_min) {\n            epsilon *= epsilon_decay;\n        }\n        \n        // Update target network periodically\n        if (episode % 100 == 0) {\n            updateTargetNetwork();\n        }\n        \n        if (episode % 50 == 0) {\n            std::cout << \"Episode \" << episode << \", Score: \" << game.getScore() \n                     << \", Total Reward: \" << total_reward \n                     << \", Epsilon: \" << epsilon << std::endl;\n        }\n    }\n}\n\nvoid DQNTrainer::saveModel(const std::string& path) {\n    network->save(path);\n}\n\nvoid DQNTrainer::loadModel(const std::string& path) {\n    network->load(path);\n}\n\nvoid DQNTrainer::updateTargetNetwork() {\n    auto source_params = network->named_parameters();\n    auto target_params = target_network->named_parameters();\n    torch::NoGradGuard no_grad;\n    for (auto& pair : source_params) {\n        target_params[pair.key()].copy_(pair.value());\n    }\n}\n\nvoid DQNTrainer::replayTraining() {\n    if (replay_buffer.size() < batch_size) return;\n    \n    auto device = network->parameters()[0].device();\n    \n    // Sample random batch\n    std::vector<int> indices(replay_buffer.size());\n    std::iota(indices.begin(), indices.end(), 0);\n    std::shuffle(indices.begin(), indices.end(), std::default_random_engine{});\n    \n    std::vector<std::vector<float>> states, next_states;\n    std::vector<int> actions;\n    std::vector<float> rewards;\n    std::vector<bool> dones;\n    \n    for (int i = 0; i < batch_size; i++) {\n        auto& exp = replay_buffer[indices[i]];\n        states.push_back(exp.state);\n        next_states.push_back(exp.next_state);\n        actions.push_back(exp.action);\n        rewards.push_back(exp.reward);\n        dones.push_back(exp.done);\n    }\n    \n    // Convert to tensors\n    auto state_tensor = torch::zeros({batch_size, static_cast<long>(states[0].size())});\n    auto next_state_tensor = torch::zeros({batch_size, static_cast<long>(next_states[0].size())});\n    \n    for (int i = 0; i < batch_size; i++) {\n        state_tensor[i] = torch::from_blob(states[i].data(), {static_cast<long>(states[i].size())}, torch::kFloat);\n        next_state_tensor[i] = torch::from_blob(next_states[i].data(), {static_cast<long>(next_states[i].size())}, torch::kFloat);\n    }\n    \n    state_tensor = state_tensor.to(device);\n    next_state_tensor = next_state_tensor.to(device);\n    auto action_tensor = torch::from_blob(actions.data(), {batch_size}, torch::kLong).to(device);\n    auto reward_tensor = torch::from_blob(rewards.data(), {batch_size}, torch::kFloat).to(device);\n    \n    // Compute Q-values\n    auto current_q_values = network->forward(state_tensor).gather(1, action_tensor.unsqueeze(1));\n    \n    torch::Tensor next_q_values;\n    {\n        torch::NoGradGuard no_grad;\n        next_q_values = std::get<0>(target_network->forward(next_state_tensor).max(1));\n    }\n    \n    auto target_q_values = reward_tensor + gamma * next_q_values;\n    \n    // Mask done states\n    for (int i = 0; i < batch_size; i++) {\n        if (dones[i]) {\n            target_q_values[i] = reward_tensor[i];\n        }\n    }\n    \n    // Compute loss\n    auto loss = torch::mse_loss(current_q_values.squeeze(), target_q_values);\n    \n    // Backpropagation\n    optimizer->zero_grad();\n    loss.backward();\n    optimizer->step();\n}\n\nvoid DQNTrainer::addExperience(const Experience& exp) {\n    replay_buffer.push_back(exp);\n    if (replay_buffer.size() > max_buffer_size) {\n        replay_buffer.erase(replay_buffer.begin());\n    }\n}\n
#include \"neural_network.h\"\r\n#include <iostream>\r\n#include <random>\r\n#include <numeric>\r\n#include <algorithm>\r\n#include \"snake_logic.h\"\r\n\r\nSnakeNeuralNetworkImpl::SnakeNeuralNetworkImpl(int input_size, int hidden_size, int output_size) \r\n    : fc1(register_module(\"fc1\", torch::nn::Linear(input_size, hidden_size))),\r\n      fc2(register_module(\"fc2\", torch::nn::Linear(hidden_size, hidden_size))),\r\n      fc3(register_module(\"fc3\", torch::nn::Linear(hidden_size, output_size))),\r\n      rng(std::random_device{}()) {\r\n}\r\n\r\ntorch::Tensor SnakeNeuralNetworkImpl::forward(torch::Tensor x) {\r\n    x = torch::relu(fc1->forward(x));\r\n    x = torch::relu(fc2->forward(x));\r\n    x = fc3->forward(x);\r\n    return x;\r\n}\r\n\r\ntorch::Tensor SnakeNeuralNetworkImpl::getAction(const std::vector<float>& state, float epsilon) {\r\n    if (std::uniform_real_distribution<float>(0.0f, 1.0f)(rng) < epsilon) {\r\n        // Random action\r\n        return torch::randint(0, 4, {1});\r\n    }\r\n    \r\n    // Neural network action\r\n    torch::NoGradGuard no_grad;\r\n    auto state_tensor = torch::from_blob(const_cast<float*>(state.data()), \r\n                                       {1, static_cast<long>(state.size())}, \r\n                                       torch::kFloat);\r\n    auto q_values = forward(state_tensor);\r\n    return torch::argmax(q_values, 1);\r\n}\r\n\r\nvoid SnakeNeuralNetworkImpl::save(const std::string& path) {\r\n    torch::save(this->parameters(), path);\r\n}\r\n\r\nvoid SnakeNeuralNetworkImpl::load(const std::string& path) {\r\n    std::vector<torch::Tensor> params;\r\n    torch::load(params, path);\r\n    \r\n    auto model_params = this->parameters();\r\n    for (size_t i = 0; i < params.size() && i < model_params.size(); i++) {\r\n        model_params[i].data().copy_(params[i].data());\r\n    }\r\n}\r\n\r\nDQNTrainer::DQNTrainer(int input_size, int hidden_size, int output_size, \r\n                       float learning_rate, float gamma_param) {\r\n    gamma = gamma_param;\r\n    \r\n    // Initialize networks using register_module\r\n    network = register_module(\"network\", SnakeNeuralNetwork(input_size, hidden_size, output_size));\r\n    target_network = register_module(\"target_network\", SnakeNeuralNetwork(input_size, hidden_size, output_size));\r\n    \r\n    // Initialize target network with same weights\r\n    auto source_params = network->named_parameters();\r\n    auto target_params = target_network->named_parameters();\r\n    torch::NoGradGuard no_grad;\r\n    for (auto& pair : source_params) {\r\n        target_params[pair.key()].copy_(pair.value());\r\n    }\r\n    \r\n    // Setup optimizer\r\n    optimizer = std::make_unique<torch::optim::Adam>(network->parameters(), torch::optim::AdamOptions(learning_rate));\r\n}\r\n\r\nvoid DQNTrainer::train(int episodes) {\r\n    // Force CPU for Release builds to avoid CUDA issues on Windows\r\n#ifdef NDEBUG\r\n    auto device = torch::kCPU;\r\n    std::cout << \"Release build: Using CPU only to avoid CUDA issues\" << std::endl;\r\n#else\r\n    auto device = torch::cuda::is_available() ? torch::kCUDA : torch::kCPU;\r\n    std::cout << \"Debug build: Training on: \" << (device == torch::kCUDA ? \"CUDA\" : \"CPU\") << std::endl;\r\n#endif\r\n    \r\n    network->to(device);\r\n    target_network->to(device);\r\n    \r\n    for (int episode = 0; episode < episodes; episode++) {\r\n        SnakeGame game;\r\n        auto state = game.getGameState();\r\n        float total_reward = 0.0f;\r\n        int steps = 0;\r\n        \r\n        while (!game.isGameOver() && steps < 1000) {\r\n            // Get action from network\r\n            auto action_tensor = network->getAction(state, epsilon);\r\n            int action = action_tensor.item<int>();\r\n            \r\n            // Convert action to direction\r\n            Direction dir = static_cast<Direction>(action);\r\n            game.setDirection(dir);\r\n            \r\n            // Take step\r\n            game.update();\r\n            auto next_state = game.getGameState();\r\n            float reward = game.getReward();\r\n            bool done = game.isGameOver();\r\n            \r\n            // Store experience\r\n            addExperience({state, action, reward, next_state, done});\r\n            \r\n            state = next_state;\r\n            total_reward += reward;\r\n            steps++;\r\n            \r\n            // Train network\r\n            if (replay_buffer.size() >= batch_size) {\r\n                replayTraining();\r\n            }\r\n        }\r\n        \r\n        // Decay epsilon\r\n        if (epsilon > epsilon_min) {\r\n            epsilon *= epsilon_decay;\r\n        }\r\n        \r\n        // Update target network periodically\r\n        if (episode % 100 == 0) {\r\n            updateTargetNetwork();\r\n        }\r\n        \r\n        if (episode % 50 == 0) {\r\n            std::cout << \"Episode \" << episode << \", Score: \" << game.getScore() \r\n                     << \", Total Reward: \" << total_reward \r\n                     << \", Epsilon: \" << epsilon << std::endl;\r\n        }\r\n    }\r\n}\r\n\r\nvoid DQNTrainer::saveModel(const std::string& path) {\r\n    network->save(path);\r\n}\r\n\r\nvoid DQNTrainer::loadModel(const std::string& path) {\r\n    network->load(path);\r\n}\r\n\r\nvoid DQNTrainer::updateTargetNetwork() {\r\n    auto source_params = network->named_parameters();\r\n    auto target_params = target_network->named_parameters();\r\n    torch::NoGradGuard no_grad;\r\n    for (auto& pair : source_params) {\r\n        target_params[pair.key()].copy_(pair.value());\r\n    }\r\n}\r\n\r\nvoid DQNTrainer::replayTraining() {\r\n    if (replay_buffer.size() < batch_size) return;\r\n    \r\n    auto device = network->parameters()[0].device();\r\n    \r\n    // Sample random batch\r\n    std::vector<int> indices(replay_buffer.size());\r\n    std::iota(indices.begin(), indices.end(), 0);\r\n    std::shuffle(indices.begin(), indices.end(), std::default_random_engine{});\r\n    \r\n    std::vector<std::vector<float>> states, next_states;\r\n    std::vector<int> actions;\r\n    std::vector<float> rewards;\r\n    std::vector<bool> dones;\r\n    \r\n    for (int i = 0; i < batch_size; i++) {\r\n        auto& exp = replay_buffer[indices[i]];\r\n        states.push_back(exp.state);\r\n        next_states.push_back(exp.next_state);\r\n        actions.push_back(exp.action);\r\n        rewards.push_back(exp.reward);\r\n        dones.push_back(exp.done);\r\n    }\r\n    \r\n    // Convert to tensors\r\n    auto state_tensor = torch::zeros({batch_size, static_cast<long>(states[0].size())});\r\n    auto next_state_tensor = torch::zeros({batch_size, static_cast<long>(next_states[0].size())});\r\n    \r\n    for (int i = 0; i < batch_size; i++) {\r\n        state_tensor[i] = torch::from_blob(states[i].data(), {static_cast<long>(states[i].size())}, torch::kFloat);\r\n        next_state_tensor[i] = torch::from_blob(next_states[i].data(), {static_cast<long>(next_states[i].size())}, torch::kFloat);\r\n    }\r\n    \r\n    state_tensor = state_tensor.to(device);\r\n    next_state_tensor = next_state_tensor.to(device);\r\n    auto action_tensor = torch::from_blob(actions.data(), {batch_size}, torch::kLong).to(device);\r\n    auto reward_tensor = torch::from_blob(rewards.data(), {batch_size}, torch::kFloat).to(device);\r\n    \r\n    // Compute Q-values\r\n    auto current_q_values = network->forward(state_tensor).gather(1, action_tensor.unsqueeze(1));\r\n    \r\n    torch::Tensor next_q_values;\r\n    {\r\n        torch::NoGradGuard no_grad;\r\n        next_q_values = std::get<0>(target_network->forward(next_state_tensor).max(1));\r\n    }\r\n    \r\n    auto target_q_values = reward_tensor + gamma * next_q_values;\r\n    \r\n    // Mask done states\r\n    for (int i = 0; i < batch_size; i++) {\r\n        if (dones[i]) {\r\n            target_q_values[i] = reward_tensor[i];\r\n        }\r\n    }\r\n    \r\n    // Compute loss\r\n    auto loss = torch::mse_loss(current_q_values.squeeze(), target_q_values);\r\n    \r\n    // Backpropagation\r\n    optimizer->zero_grad();\r\n    loss.backward();\r\n    optimizer->step();\r\n}\r\n\r\nvoid DQNTrainer::addExperience(const Experience& exp) {\r\n    replay_buffer.push_back(exp);\r\n    if (replay_buffer.size() > max_buffer_size) {\r\n        replay_buffer.erase(replay_buffer.begin());\r\n    }\r\n}\r\n